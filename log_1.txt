balu@balu-GL702VMK:~/Work/OnlineNewsPopularity$ python model.py
MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(64,), learning_rate='adaptive',
       learning_rate_init=0.01, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
/usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
0.151234997112 0.0750116541713
0.152946654764 0.0743086540521
0.146285768398 0.0789037564759
0.153295051339 0.0745224456647
[0.15123499711231708, 0.15294665476441668, 0.14628576839829277, 0.15329505133897509]
[0.07501165417126565, 0.07430865405212428, 0.07890375647592651, 0.07452244566471122]
0.150940617904 0.075686627591
0.150940617904 0.075686627591
MLPRegressor(activation='tanh', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(64, 64), learning_rate='adaptive',
       learning_rate_init=0.01, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
0.15045992449 0.0797521782346
0.152727738782 0.0756336168663
0.146176850931 0.0795895611933
0.153057268316 0.0759579965775
[0.15045992448976936, 0.1527277387823247, 0.14617685093091914, 0.15305726831566474]
[0.07975217823458891, 0.07563361686627912, 0.07958956119329885, 0.07595799657750035]
0.15060544563 0.0777333382179
0.15060544563 0.0777333382179
MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(64, 64), learning_rate='adaptive',
       learning_rate_init=0.01, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='sgd', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
0.149047070312 0.0883935223279
0.151821734943 0.0811170968044
0.146450946552 0.0778637033092
0.150664047696 0.0904064210181
[0.14904707031177636, 0.1518217349434858, 0.1464509465517619, 0.15066404769566122]
[0.08839352232790343, 0.0811170968044348, 0.07786370330917936, 0.09040642101807783]
0.149495949876 0.0844451858649
0.149495949876 0.0844451858649
MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(64, 64), learning_rate='adaptive',
       learning_rate_init=0.01, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
0.153849610571 0.0590200713755
0.150148899036 0.091241736175
0.140618400305 0.114588645841
0.148144247335 0.105619036528
[0.1538496105707943, 0.15014889903636, 0.14061840030548112, 0.14814424733451992]
[0.059020071375504024, 0.09124173617495535, 0.1145886458408576, 0.10561903652831717]
0.148190289312 0.0926173724799
0.148190289312 0.0926173724799
MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(128, 128), learning_rate='adaptive',
       learning_rate_init=0.001, max_iter=10000, momentum=0.9,
       nesterovs_momentum=True, power_t=0.5, random_state=None,
       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,
       verbose=False, warm_start=False)
0.144146237133 0.11836815559
0.151055114471 0.0857569755805
0.146100613262 0.0800695957968
0.153081668487 0.0758106871204
[0.1441462371334755, 0.15105511447067085, 0.14610061326158905, 0.15308166848688548]
[0.11836815558962421, 0.08575697558049789, 0.08006959579682771, 0.07581068712041406]
0.148595908338 0.0900013535218
0.148595908338 0.0900013535218
